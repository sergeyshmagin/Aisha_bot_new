"""
–°–µ—Ä–≤–∏—Å –∞–Ω–∞–ª–∏–∑–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –ø—Ä–æ–º–ø—Ç–æ–≤
–ò—Å–ø–æ–ª—å–∑—É–µ—Ç OpenAI Vision API –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —Ä–µ—Ñ–µ—Ä–µ–Ω—Å–Ω—ã—Ö —Ñ–æ—Ç–æ
"""
import aiohttp
import base64
from typing import Optional, Dict, Any
import io
from PIL import Image

from app.core.config import settings
from app.core.logger import get_logger
from app.shared.utils.openai import get_openai_headers
from .cinematic_prompt_service import CinematicPromptService

logger = get_logger(__name__)


class ImageAnalysisService:
    """–°–µ—Ä–≤–∏—Å –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏ —Å–æ–∑–¥–∞–Ω–∏—è –ø—Ä–æ–º–ø—Ç–æ–≤"""
    
    def __init__(self):
        self.openai_api_key = settings.OPENAI_API_KEY
        self.model = "gpt-4o"  # GPT-4 —Å Vision
        self.max_image_size = 2048  # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –¥–ª—è API
        self.cinematic_service = CinematicPromptService()
    
    async def analyze_image_for_prompt(
        self, 
        image_data: bytes, 
        avatar_type: str = "portrait",
        user_prompt: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∏ —Å–æ–∑–¥–∞–µ—Ç –∫–∏–Ω–µ–º–∞—Ç–æ–≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏–π –ø—Ä–æ–º–ø—Ç
        
        Args:
            image_data: –î–∞–Ω–Ω—ã–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
            avatar_type: –¢–∏–ø –∞–≤–∞—Ç–∞—Ä–∞ ("portrait")
            user_prompt: –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –ø—Ä–æ–º–ø—Ç –æ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –¥–ª—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏
            
        Returns:
            Dict —Å –∞–Ω–∞–ª–∏–∑–æ–º –∏ –≥–æ—Ç–æ–≤—ã–º –ø—Ä–æ–º–ø—Ç–æ–º
        """
        try:
            logger.info(f"[Image Analysis] –ù–∞—á–∞—Ç –∞–Ω–∞–ª–∏–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è ({len(image_data)} –±–∞–π—Ç)")
            
            if not self.openai_api_key:
                logger.warning("[Image Analysis] –ù–µ—Ç API –∫–ª—é—á–∞ OpenAI, –∏—Å–ø–æ–ª—å–∑—É–µ–º –±–∞–∑–æ–≤—ã–π –ø—Ä–æ–º–ø—Ç")
                return await self._fallback_analysis(user_prompt, avatar_type)
            
            # 1. –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
            processed_image = await self._prepare_image(image_data)
            if not processed_image:
                return await self._fallback_analysis(user_prompt, avatar_type)
            
            # 2. –ö–æ–¥–∏—Ä—É–µ–º –≤ base64
            image_base64 = base64.b64encode(processed_image).decode('utf-8')
            
            # 3. –°–æ–∑–¥–∞–µ–º —Å–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
            system_prompt = self._create_cinematic_analysis_prompt(avatar_type, user_prompt)
            
            # 4. –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —á–µ—Ä–µ–∑ GPT Vision
            analysis_result = await self._call_vision_api(system_prompt, image_base64, user_prompt)
            
            if not analysis_result or not analysis_result.get("prompt"):
                logger.warning("[Image Analysis] –ù–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –æ—Ç Vision API, –∏—Å–ø–æ–ª—å–∑—É–µ–º fallback")
                return await self._fallback_analysis(user_prompt, avatar_type)
            
            # 5. –°–æ–∑–¥–∞–µ–º –∫–∏–Ω–µ–º–∞—Ç–æ–≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏–π –ø—Ä–æ–º–ø—Ç –Ω–∞ –±–∞–∑–µ –∞–Ω–∞–ª–∏–∑–∞
            base_description = analysis_result.get("analysis", "")
            vision_prompt = analysis_result.get("prompt", "")
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ–± –æ–∫—Ä—É–∂–µ–Ω–∏–∏ –∏–∑ –∞–Ω–∞–ª–∏–∑–∞
            environment_text = self._extract_environment_from_analysis(base_description, vision_prompt)
            
            # –ò–Ω—Ç–µ–≥—Ä–∏—Ä—É–µ–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–π –ø—Ä–æ–º–ø—Ç –µ—Å–ª–∏ –µ—Å—Ç—å
            if user_prompt:
                integrated_prompt = f"{vision_prompt}, {user_prompt}"
            else:
                integrated_prompt = vision_prompt
            
            # 6. –ü—Ä–∏–º–µ–Ω—è–µ–º –∫–∏–Ω–µ–º–∞—Ç–æ–≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏–µ —É–ª—É—á—à–µ–Ω–∏—è —Å environment_text
            cinematic_result = await self.cinematic_service.create_cinematic_prompt(
                user_prompt=integrated_prompt,
                avatar_type=avatar_type,
                style_preset="photorealistic",
                environment_text=environment_text
            )
            
            logger.info(f"[Image Analysis] –°–æ–∑–¥–∞–Ω –∫–∏–Ω–µ–º–∞—Ç–æ–≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏–π –ø—Ä–æ–º–ø—Ç: {len(cinematic_result['processed'])} —Å–∏–º–≤–æ–ª–æ–≤")
            
            # –î–æ–±–∞–≤–ª—è–µ–º –Ω–µ–≥–∞—Ç–∏–≤–Ω—ã–π –ø—Ä–æ–º–ø—Ç —Å —É–ª—É—á—à–µ–Ω–∏—è–º–∏ –¥–ª—è –≥–ª–∞–∑
            from app.services.generation.prompt.enhancement.prompt_enhancer import PromptEnhancer
            enhancer = PromptEnhancer()
            negative_prompt = enhancer.get_negative_prompt(avatar_type)
            
            return {
                "analysis": base_description,
                "prompt": cinematic_result["processed"],
                "negative_prompt": negative_prompt,
                "original_vision_prompt": vision_prompt,
                "cinematic_enhancement": cinematic_result.get("enhancement_applied", False),
                "user_prompt_integrated": bool(user_prompt),
                "style": "cinematic_photorealistic"
            }
                
        except Exception as e:
            logger.exception(f"[Image Analysis] –û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞: {e}")
            return await self._fallback_analysis(user_prompt, avatar_type)
    
    def _create_cinematic_analysis_prompt(self, avatar_type: str, user_prompt: str = None) -> str:
        """–°–æ–∑–¥–∞–µ—Ç —Å–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç –¥–ª—è –∫–∏–Ω–µ–º–∞—Ç–æ–≥—Ä–∞—Ñ–∏—á–µ—Å–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞"""
        
        user_integration = ""
        if user_prompt:
            user_integration = f"""
üéØ USER REQUEST INTEGRATION:
User added text request: "{user_prompt}"

IMPORTANT: Combine visual composition from photo with user request content.
- PRESERVE: shot type, pose, lighting, atmosphere from photo
- INTEGRATE: request content (suit, scene, character, style)"""
        
        return f"""You are an expert in image analysis for creating CINEMATIC prompts in professional 8K photography style.

TASK: Analyze the image and create a prompt in example style with maximum detail.{user_integration}

üé¨ PROMPT STYLE (like examples):
Must include ALL elements:
1. **Technical specs**: "high-quality, cinematic, ultra-realistic", "8K resolution", "professional camera"
2. **Shot type**: "close-up portrait"/"full-body portrait"/"medium portrait" 
3. **Lighting**: "warm directional side lighting during golden hour" / "professional studio lighting"
4. **Composition**: "expertly framed with subject positioned centrally"
5. **Detailed subject description**: appearance, clothing, style, expression
6. **Pose and angle**: exact body position, gaze direction, gestures
7. **Environment**: detailed background description, atmosphere, context
8. **Technical parameters**: "shot with 85mm lens", "depth of field", "razor-sharp focus"
9. **Color palette**: "rich warm tones, deep golds, luxurious ambers"
10. **Quality**: "natural skin texture", "well-defined eyes", "authentic detail"

üìê SHOT TYPE DETERMINATION (CRITICALLY IMPORTANT):
- Are person's legs visible? ‚Üí FULL-BODY PORTRAIT
- Torso visible to waist? ‚Üí HALF-BODY PORTRAIT  
- Only head and shoulders? ‚Üí CLOSE-UP PORTRAIT

üîç ANALYSIS BY BLOCKS:

**COMPOSITION AND FRAME:**
- What exact shot type (full body/half body/close-up)?
- How is subject positioned in frame?
- Shooting angles and perspective

**LIGHTING:**
- Lighting type (studio/natural/golden hour/dramatic)
- Light direction and shadows
- Atmosphere and mood

**SUBJECT:**
- Detailed appearance description
- Clothing and style (colors, textures, details)
- Facial expression and emotions

**POSE AND BODY LANGUAGE:**
- Exact body position
- Gaze direction
- Gestures and hand positions

**ENVIRONMENT:**
- Detailed background description
- Context and location
- Interior/exterior elements
- Identify recognizable cities or landmarks if possible, specify them in analysis

**TECHNICAL DETAILS:**
- Depth of field
- Focus
- Image quality

JSON RESPONSE FORMAT:
```json
{{
  "analysis": "Detailed analysis of each composition block",
  "prompt": "Ready cinematic prompt in example style"
}}
```

EXAMPLE PROMPT STYLE:
"A high-quality, cinematic, ultra-realistic close-up portrait photograph, captured by professional medium-format digital camera, in style of super-detailed 8K resolution imagery, featuring warm directional side lighting during golden hour. The composition is expertly framed with subject positioned centrally, featuring a confident man with contemporary styling, positioned with natural elegance and authentic body language, gazing directly at camera with engaging intensity. Set in sophisticated modern environment with clean architectural lines, captured by professional medium-format digital camera, shot with 85mm portrait lens at f/2.8 for optimal sharpness, The depth of field is exceptional ensuring razor-sharp focus on subject, professional bokeh with smooth background transition. The color palette emphasizes rich warm tones and deep golds creating sophisticated atmospheric mood, beautiful detailed eyes with sharp pupils, clean eyelashes, realistic reflection, well-defined eyes with natural catchlight and authentic iris detail, natural skin texture with fine detail and visible pores, sharp focus with optimal detail retention, high-end editorial photography style with cinematic quality."

Create prompt EXACTLY in this style with maximum detail!"""
    
    async def _prepare_image(self, image_data: bytes) -> Optional[bytes]:
        """–ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞"""
        try:
            # –û—Ç–∫—Ä—ã–≤–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
            image = Image.open(io.BytesIO(image_data))
            
            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ RGB –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
            if image.mode != 'RGB':
                image = image.convert('RGB')
            
            # –ò–∑–º–µ–Ω—è–µ–º —Ä–∞–∑–º–µ—Ä –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
            if max(image.size) > self.max_image_size:
                ratio = self.max_image_size / max(image.size)
                new_size = tuple(int(dim * ratio) for dim in image.size)
                image = image.resize(new_size, Image.Resampling.LANCZOS)
                logger.info(f"[Image Analysis] –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∏–∑–º–µ–Ω–µ–Ω–æ –¥–æ {new_size}")
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –±–∞–π—Ç—ã
            output = io.BytesIO()
            image.save(output, format='JPEG', quality=85)
            return output.getvalue()
            
        except Exception as e:
            logger.error(f"[Image Analysis] –û—à–∏–±–∫–∞ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: {e}")
            return None
    
    async def _call_vision_api(self, system_prompt: str, image_base64: str, user_prompt: str = None) -> Dict[str, Any]:
        """–í—ã–∑—ã–≤–∞–µ—Ç OpenAI Vision API"""
        try:
            url = "https://api.openai.com/v1/chat/completions"
            headers = get_openai_headers(self.openai_api_key)
            
            user_message_text = "Analyze this image and create cinematic prompt in example style. Response in JSON format."
            if user_prompt:
                user_message_text = f"""Analyze image and create cinematic prompt, INTEGRATING user request.

USER REQUEST: "{user_prompt}"

Combine EXACT photo composition with REQUEST content. Response in JSON format."""
            
            data = {
                "model": self.model,
                "messages": [
                    {"role": "system", "content": system_prompt},
                    {
                        "role": "user",
                        "content": [
                            {"type": "text", "text": user_message_text},
                            {
                                "type": "image_url",
                                "image_url": {
                                    "url": f"data:image/jpeg;base64,{image_base64}",
                                    "detail": "high"
                                }
                            }
                        ]
                    }
                ],
                "temperature": 0.1,
                "max_tokens": 1000
            }
            
            async with aiohttp.ClientSession() as session:
                async with session.post(url, headers=headers, json=data, timeout=30) as response:
                    if response.status == 200:
                        result = await response.json()
                        content = result["choices"][0]["message"]["content"]
                        
                        # –ü–∞—Ä—Å–∏–º JSON –æ—Ç–≤–µ—Ç
                        try:
                            import json
                            parsed_result = json.loads(content)
                            return parsed_result
                        except json.JSONDecodeError:
                            # –ï—Å–ª–∏ –Ω–µ JSON, –ø—ã—Ç–∞–µ–º—Å—è –∏–∑–≤–ª–µ—á—å JSON –∏–∑ markdown –±–ª–æ–∫–∞
                            logger.warning("[Vision API] –û—Ç–≤–µ—Ç –Ω–µ –≤ JSON, –∏–∑–≤–ª–µ–∫–∞–µ–º –∏–∑ markdown")
                            
                            # –ü—Ä–æ–±—É–µ–º –∏–∑–≤–ª–µ—á—å JSON –∏–∑ markdown –±–ª–æ–∫–∞ ```json...```
                            import re
                            json_match = re.search(r'```json\s*\n(.*?)\n```', content, re.DOTALL)
                            if json_match:
                                try:
                                    json_content = json_match.group(1)
                                    parsed_result = json.loads(json_content)
                                    logger.info(f"[Vision API] JSON —É—Å–ø–µ—à–Ω–æ –∏–∑–≤–ª–µ—á–µ–Ω –∏–∑ markdown –±–ª–æ–∫–∞")
                                    return parsed_result
                                except json.JSONDecodeError:
                                    logger.warning("[Vision API] –ù–µ —É–¥–∞–ª–æ—Å—å –ø–∞—Ä—Å–∏—Ç—å JSON –∏–∑ markdown")
                            
                            # –ï—Å–ª–∏ –∏ —ç—Ç–æ –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç, –ø—ã—Ç–∞–µ–º—Å—è –∏–∑–≤–ª–µ—á—å –ø—Ä–æ–º–ø—Ç –∏–∑ —Ç–µ–∫—Å—Ç–∞
                            # –ò—â–µ–º —Å—Ç—Ä–æ–∫—É —Å "prompt": "..."
                            prompt_match = re.search(r'"prompt":\s*"([^"]+)"', content)
                            if prompt_match:
                                extracted_prompt = prompt_match.group(1)
                                return {
                                    "analysis": "–ê–Ω–∞–ª–∏–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –≤—ã–ø–æ–ª–Ω–µ–Ω",
                                    "prompt": extracted_prompt
                                }
                            
                            # –ü–æ—Å–ª–µ–¥–Ω—è—è –ø–æ–ø—ã—Ç–∫–∞ - –∏—Å–ø–æ–ª—å–∑—É–µ–º –≤–µ—Å—å –∫–æ–Ω—Ç–µ–Ω—Ç –∫–∞–∫ –ø—Ä–æ–º–ø—Ç
                            return {
                                "analysis": "–ê–Ω–∞–ª–∏–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –≤—ã–ø–æ–ª–Ω–µ–Ω", 
                                "prompt": content.strip()
                            }
                    else:
                        error_text = await response.text()
                        logger.error(f"[Vision API] –û—à–∏–±–∫–∞ {response.status}: {error_text}")
                        return {}
                        
        except Exception as e:
            logger.error(f"[Vision API] –û—à–∏–±–∫–∞ –≤—ã–∑–æ–≤–∞ API: {e}")
            return {}
    
    async def _fallback_analysis(self, user_prompt: Optional[str], avatar_type: str) -> Dict[str, Any]:
        """Fallback –∞–Ω–∞–ª–∏–∑ –±–µ–∑ GPT Vision"""
        logger.info("[Image Analysis] –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ fallback –∞–Ω–∞–ª–∏–∑–∞")
        
        # –°–æ–∑–¥–∞–µ–º –Ω–µ–≥–∞—Ç–∏–≤–Ω—ã–π –ø—Ä–æ–º–ø—Ç
        from app.services.generation.prompt.enhancement.prompt_enhancer import PromptEnhancer
        enhancer = PromptEnhancer()
        negative_prompt = enhancer.get_negative_prompt(avatar_type)
        
        if user_prompt:
            # –°–æ–∑–¥–∞–µ–º –∫–∏–Ω–µ–º–∞—Ç–æ–≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏–π –ø—Ä–æ–º–ø—Ç –Ω–∞ –±–∞–∑–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞
            cinematic_result = await self.cinematic_service.create_cinematic_prompt(
                user_prompt=user_prompt,
                avatar_type=avatar_type,
                style_preset="photorealistic",
                environment_text=None  # –í fallback —Ä–µ–∂–∏–º–µ environment_text –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω
            )
            
            return {
                "analysis": f"–ë–∞–∑–æ–≤—ã–π –∞–Ω–∞–ª–∏–∑: {user_prompt}",
                "prompt": cinematic_result["processed"],
                "negative_prompt": negative_prompt,
                "cinematic_enhancement": True,
                "style": "cinematic_fallback"
            }
        else:
            # –ë–∞–∑–æ–≤—ã–π –∫–∏–Ω–µ–º–∞—Ç–æ–≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏–π –ø—Ä–æ–º–ø—Ç
            base_prompt = "professional portrait"
            cinematic_result = await self.cinematic_service.create_cinematic_prompt(
                user_prompt=base_prompt,
                avatar_type=avatar_type,
                style_preset="photorealistic",
                environment_text=None  # –í fallback —Ä–µ–∂–∏–º–µ environment_text –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω
            )
            
            return {
                "analysis": "–ë–∞–∑–æ–≤—ã–π –∞–Ω–∞–ª–∏–∑ –ø–æ—Ä—Ç—Ä–µ—Ç–∞",
                "prompt": cinematic_result["processed"],
                "negative_prompt": negative_prompt,
                "cinematic_enhancement": True,
                "style": "cinematic_default"
            }

    def is_available(self) -> bool:
        """
        –ü—Ä–æ–≤–µ—Ä—è–µ—Ç –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å —Å–µ—Ä–≤–∏—Å–∞ –∞–Ω–∞–ª–∏–∑–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
            
        Returns:
            bool: True –µ—Å–ª–∏ —Å–µ—Ä–≤–∏—Å –¥–æ—Å—Ç—É–ø–µ–Ω (–µ—Å—Ç—å OpenAI API –∫–ª—é—á), False –µ—Å–ª–∏ —Ä–∞–±–æ—Ç–∞–µ—Ç –≤ fallback —Ä–µ–∂–∏–º–µ
        """
        # –°–µ—Ä–≤–∏—Å –≤—Å–µ–≥–¥–∞ –¥–æ—Å—Ç—É–ø–µ–Ω, –Ω–æ —Ä–µ–∂–∏–º –∑–∞–≤–∏—Å–∏—Ç –æ—Ç –Ω–∞–ª–∏—á–∏—è API –∫–ª—é—á–∞
        is_vision_available = bool(self.openai_api_key)
        
        if is_vision_available:
            logger.debug("[Image Analysis] –°–µ—Ä–≤–∏—Å –¥–æ—Å—Ç—É–ø–µ–Ω —Å GPT-4 Vision API")
        else:
            logger.debug("[Image Analysis] –°–µ—Ä–≤–∏—Å –¥–æ—Å—Ç—É–ø–µ–Ω –≤ fallback —Ä–µ–∂–∏–º–µ (–±–µ–∑ Vision API)")
        
        return True  # –°–µ—Ä–≤–∏—Å –≤—Å–µ–≥–¥–∞ –¥–æ—Å—Ç—É–ø–µ–Ω (fallback –≤ —Å–ª—É—á–∞–µ –æ—Ç—Å—É—Ç—Å—Ç–≤–∏—è API –∫–ª—é—á–∞) 

    def _extract_environment_from_analysis(self, analysis: str, prompt: str) -> Optional[str]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ–± –æ–∫—Ä—É–∂–µ–Ω–∏–∏ –∏–∑ –∞–Ω–∞–ª–∏–∑–∞ GPT Vision"""
        import re
        
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º –∞–Ω–∞–ª–∏–∑ –∏ –ø—Ä–æ–º–ø—Ç –¥–ª—è –ø–æ–∏—Å–∫–∞
        combined_text = f"{analysis} {prompt}".lower()
        
        # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –ø–æ–∏—Å–∫–∞ –æ–ø–∏—Å–∞–Ω–∏–π –æ–∫—Ä—É–∂–µ–Ω–∏—è
        environment_patterns = [
            # –ò—â–µ–º —É–ø–æ–º–∏–Ω–∞–Ω–∏—è –∏–∑–≤–µ—Å—Ç–Ω—ã—Ö –º–µ—Å—Ç
            r'(?:dubai|burj khalifa|–¥—É–±–∞–π|–±—É—Ä–¥–∂ —Ö–∞–ª–∏—Ñ–∞)',
            r'(?:moscow|red square|–º–æ—Å–∫–≤–∞|–∫—Ä–∞—Å–Ω–∞—è –ø–ª–æ—â–∞–¥—å)',
            r'(?:new york|times square|–Ω—å—é[-\s]?–π–æ—Ä–∫|—Ç–∞–π–º—Å[-\s]?—Å–∫–≤–µ—Ä)',
            r'(?:london|big ben|–ª–æ–Ω–¥–æ–Ω|–±–∏–≥[-\s]?–±–µ–Ω)',
            r'(?:paris|eiffel tower|–ø–∞—Ä–∏–∂|—ç–π—Ñ–µ–ª–µ–≤–∞ –±–∞—à–Ω—è)',
            
            # –ò—â–µ–º –æ–ø–∏—Å–∞–Ω–∏—è —Ç–∏–ø–æ–≤ –ª–æ–∫–∞—Ü–∏–π
            r'(?:office|–æ—Ñ–∏—Å|business|–¥–µ–ª–æ–≤–æ–π)',
            r'(?:studio|—Å—Ç—É–¥–∏—è|photography studio)',
            r'(?:restaurant|cafe|—Ä–µ—Å—Ç–æ—Ä–∞–Ω|–∫–∞—Ñ–µ)',
            r'(?:urban|city|–≥–æ—Ä–æ–¥—Å–∫–æ–π|–≥–æ—Ä–æ–¥)',
            r'(?:nature|forest|park|–ø—Ä–∏—Ä–æ–¥–∞|–ª–µ—Å|–ø–∞—Ä–∫)',
            r'(?:modern architecture|—Å–æ–≤—Ä–µ–º–µ–Ω–Ω–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞)',
            r'(?:skyscraper|–Ω–µ–±–æ—Å–∫—Ä–µ–±)',
            r'(?:interior|–∏–Ω—Ç–µ—Ä—å–µ—Ä)',
            r'(?:exterior|—ç–∫—Å—Ç–µ—Ä—å–µ—Ä)',
        ]
        
        found_environments = []
        
        for pattern in environment_patterns:
            matches = re.findall(pattern, combined_text, re.IGNORECASE)
            if matches:
                found_environments.extend(matches)
        
        # –ï—Å–ª–∏ –Ω–∞–π–¥–µ–Ω—ã —É–ø–æ–º–∏–Ω–∞–Ω–∏—è –æ–∫—Ä—É–∂–µ–Ω–∏—è, —Å–æ–∑–¥–∞–µ–º –æ–ø–∏—Å–∞–Ω–∏–µ
        if found_environments:
            # –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã –∏ —Å–æ—Ä—Ç–∏—Ä—É–µ–º
            unique_environments = list(set(found_environments))
            
            # –°–ø–µ—Ü–∏–∞–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–ª—è –∏–∑–≤–µ—Å—Ç–Ω—ã—Ö –º–µ—Å—Ç
            if any('dubai' in env.lower() or '–¥—É–±–∞–π' in env.lower() for env in unique_environments):
                return ("Set against the iconic Dubai skyline with the magnificent Burj Khalifa towering in the background, "
                       "featuring the architectural marvel rendered with atmospheric perspective and soft focus, "
                       "showcasing the grandeur of modern urban achievement with warm desert lighting")
                       
            elif any('office' in env.lower() or '–æ—Ñ–∏—Å' in env.lower() for env in unique_environments):
                return ("Set in a sophisticated modern office environment with clean architectural lines, "
                       "contemporary interior design elements visible in the professionally blurred background, "
                       "featuring warm ambient lighting and luxurious furnishings that convey success and professionalism")
                       
            elif any('studio' in env.lower() or '—Å—Ç—É–¥–∏—è' in env.lower() for env in unique_environments):
                return ("In a professional photography studio setting with seamless backdrop and controlled environment, "
                       "featuring expertly positioned lighting equipment and neutral tones, "
                       "creating optimal conditions for maximum image quality and focus on the subject")
                       
            elif any(env.lower() in ['urban', 'city', '–≥–æ—Ä–æ–¥—Å–∫–æ–π', '–≥–æ—Ä–æ–¥'] for env in unique_environments):
                return ("Against an urban landscape backdrop with sophisticated architectural elements softly blurred, "
                       "featuring metropolitan atmosphere with natural depth and environmental context, "
                       "showcasing the dynamic relationship between subject and contemporary cityscape")
                       
            elif any(env.lower() in ['nature', 'forest', 'park', '–ø—Ä–∏—Ä–æ–¥–∞', '–ª–µ—Å', '–ø–∞—Ä–∫'] for env in unique_environments):
                return ("Surrounded by natural landscape with organic textures and soft environmental elements, "
                       "featuring lush background with perfect depth of field and natural color harmony, "
                       "creating serene connection with the natural world and organic beauty")
                       
            elif any(env.lower() in ['restaurant', 'cafe', '—Ä–µ—Å—Ç–æ—Ä–∞–Ω', '–∫–∞—Ñ–µ'] for env in unique_environments):
                return ("Set in an elegant dining establishment with sophisticated interior design, "
                       "featuring warm ambient lighting and luxurious decor elements softly blurred in the background, "
                       "conveying refined taste and upscale lifestyle atmosphere")
        
        return None 